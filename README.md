# NLP-project
This project classifies disaster-related tweets into two categories: real disaster tweets and non-disaster tweets. The main goal was to find which machine learning algorithm provides the best accuracy for this classification task.

Why Three Algorithms?
Multinomial Naive Bayes: Well-suited for text classification problems and works quickly with word frequency data.

Logistic Regression: A robust, widely-used algorithm for binary classification problems with interpretable results.

K-Nearest Neighbors (KNN): A simple, instance-based learning algorithm that classifies based on similarity to nearest neighbors.

Using all three allows comparison of their accuracy and overall effectiveness on this dataset.

Outputs and Accuracy
Multinomial Naive Bayes achieved about 80.3% accuracy, showing strong performance in identifying disaster tweets.

Logistic Regression reached approximately 78.2% accuracy, slightly lower but still reliable.

KNN Classifier had the lowest accuracy at around 58.8%, indicating less suitability for this problem.

Confusion matrices and classification reports provided detailed precision, recall, and F1-score metrics, helping assess strengths and weaknesses of each model.
